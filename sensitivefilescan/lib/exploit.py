import traceback
import os
import urlparse
from fuzz import fuzz
from basicinfo import _requests
from basicinfo import get_site_stander
from basicinfo import get_dict_by_server
from basicinfo import get_extion_by_sever


path = os.path.realpath(os.path.dirname(__file__))


def exploit_server_path(url):
    result = []
    try:
        standers = get_site_stander(url)
        r = _requests(url)
        dicts = get_dict_by_server(r.headers)
        if dicts:
            hand = fuzz(url, dicts, standers)
            result = hand.scan()
    except:
        traceback.print_exc()
    finally:
        return result


def exploit_backup_path(url, dirs=[]):
    result = []
    try:
        domain = urlparse.urlparse(url).netloc
        standers = {
            "Content-Type": [
                "tmp", "old", "zip", "swp", "rar", "bak", "7z", "tar.gz", "tar.bz2", "tar", "iso"
            ]
        }
        preffix_path = os.path.abspath(os.path.join(path, "../dict/common/backup.preffixs"))
        suffix_path = os.path.abspath(os.path.join(path, "../dict/common/backup.suffixs"))
        if len(dirs) == 1:
            dir_path = os.path.abspath(os.path.join(path, "../dict/common/dirs.txt"))
            with open(dir_path) as f:
                commondirs = f.readlines()
                f.close()
            for i in commondirs:
                dirs.append(i.strip('\n'))
        with open(preffix_path) as f:
            preffixs = f.readlines()
            f.close()
        with open(suffix_path) as f:
            suffixs = f.readlines()
            f.close()
        dicts = []
        for d in dirs:
            for p in preffixs:
                p = p.strip('\n').format(domain)
                for s in suffixs:
                    s = s.strip('\n')
                    dicts.append(d + '/' + p + s)
        hand = fuzz(url, dicts, standers)
        result = hand.scan()
    except:
        traceback.print_exc()
    finally:
        return result


def exploit_directory_path(url, dirs=[]):
    result = []
    try:
        standers = {
            "title": r"<title>([^<]*)"
        }
        dir_path = os.path.abspath(os.path.join(path, "../dict/common/dirs.txt"))
        with open(dir_path) as f:
            commondirs = f.readlines()
            f.close()
        dicts = []
        for i in dirs:
            dicts.append(i + '/')
        for i in commondirs:
            i = i.strip("\n") + '/'
            dicts.append(i)
        hand = fuzz(url, dicts, standers)
        result = hand.scan()
    except:
        traceback.print_exc()
    finally:
        return result


def exploit_common_file(url, extion, dirs=[]):
    result = []
    try:
        dicts = []
        e = get_extion_by_sever(url)
        extion = extion if extion != 'php' else e
        standers = get_site_stander(url)
        files_path = os.path.abspath(os.path.join(path, "../dict/files.txt"))
        if len(dirs) == 1:
            dir_path = os.path.abspath(os.path.join(path, "../dict/common/dirs.txt"))
            with open(dir_path) as f:
                commondirs = f.readlines()
                f.close()
            print commondirs
            for i in commondirs:
                dirs.append(i.strip('\n'))
        with open(files_path) as f:
            files = f.readlines()
            f.close()
        for d in dirs:
            for f in files:
                f = f.strip('\n').format(extion)
                dicts.append(d + '/' + f)
        hand = fuzz(url, dicts, standers)
        result = hand.scan()
    except:
        traceback.print_exc()
    finally:
        return result
